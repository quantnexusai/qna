import { MetadataMode } from "../Node.js";
import { Response } from "../Response.js";
import { llmFromSettingsOrContext } from "../Settings.js";
import { streamConverter } from "../llm/utils.js";
import { PromptMixin } from "../prompts/Mixin.js";
import { defaultTextQaPrompt } from "./../Prompt.js";
import { createMessageContent } from "./utils.js";
export class MultiModalResponseSynthesizer extends PromptMixin {
    serviceContext;
    metadataMode;
    textQATemplate;
    constructor({ serviceContext, textQATemplate, metadataMode } = {}){
        super();
        this.serviceContext = serviceContext;
        this.metadataMode = metadataMode ?? MetadataMode.NONE;
        this.textQATemplate = textQATemplate ?? defaultTextQaPrompt;
    }
    _getPrompts() {
        return {
            textQATemplate: this.textQATemplate
        };
    }
    _updatePrompts(promptsDict) {
        if (promptsDict.textQATemplate) {
            this.textQATemplate = promptsDict.textQATemplate;
        }
    }
    async synthesize({ query, nodesWithScore, stream }) {
        const nodes = nodesWithScore.map(({ node })=>node);
        const prompt = await createMessageContent(this.textQATemplate, nodes, {
            query
        }, this.metadataMode);
        const llm = llmFromSettingsOrContext(this.serviceContext);
        if (stream) {
            const response = await llm.complete({
                prompt,
                stream
            });
            return streamConverter(response, ({ text })=>new Response(text, nodesWithScore));
        }
        const response = await llm.complete({
            prompt
        });
        return new Response(response.text, nodesWithScore);
    }
}
