import { BaseChatModel, type BaseChatModelParams } from "@langchain/core/language_models/chat_models";
import { BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
/**
 * Type representing the role of a message in the Qianfan chat model.
 */
export type QianfanRole = "assistant" | "user";
/**
 * Interface representing a message in the Qianfan chat model.
 */
interface Qianfan {
    role: QianfanRole;
    content: string;
}
/**
 * Interface representing the usage of tokens in a chat completion.
 */
interface TokenUsage {
    completionTokens?: number;
    promptTokens?: number;
    totalTokens?: number;
}
/**
 * Interface representing a request for a chat completion.
 */
interface ChatCompletionRequest {
    messages: Qianfan[];
    stream?: boolean;
    user_id?: string;
    temperature?: number;
    top_p?: number;
    penalty_score?: number;
    system?: string;
}
/**
 * Interface representing a response from a chat completion.
 */
interface ChatCompletionResponse {
    id: string;
    object: string;
    created: number;
    sentence_id: number;
    is_end: boolean;
    is_truncated: boolean;
    result: string;
    need_clear_history: boolean;
    finish_reason: string;
    usage: TokenUsage;
}
/**
 * Interface defining the input to the ChatBaiduQianfan class.
 */
declare interface BaiduQianfanChatInput {
    /**
     * Model name to use. Available options are: ERNIE-Bot, ERNIE-Lite-8K, ERNIE-Bot-4
     * Alias for `model`
     * @default "ERNIE-Bot-turbo"
     */
    modelName: string;
    /** Model name to use. Available options are: ERNIE-Bot, ERNIE-Lite-8K, ERNIE-Bot-4
     * @default "ERNIE-Bot-turbo"
     */
    model: string;
    /** Whether to stream the results or not. Defaults to false. */
    streaming?: boolean;
    /** Messages to pass as a prefix to the prompt */
    prefixMessages?: Qianfan[];
    /**
     * ID of the end-user who made requests.
     */
    userId?: string;
    /**
     * Access key to use when making requests by Qianfan SDK. Defaults to the value of
     * `QIANFAN_KEY` environment variable.
     */
    qianfanAK?: string;
    /**
     * Secret key to use when making requests by Qianfan SDK. Defaults to the value of
     * `QIANFAN_KEY` environment variable.
     */
    qianfanSK?: string;
    /**
     * Access key to use when making requests by Qianfan SDK with auth. Defaults to the value of
     * `QIANFAN_ACCESS_KEY` environment variable.
     */
    qianfanAccessKey?: string;
    /**
     * Secret key to use when making requests by Qianfan SDK with auth. Defaults to the value of
     * `QIANFAN_SECRET_KEY` environment variable.
     */
    qianfanSecretKey?: string;
    /** Amount of randomness injected into the response. Ranges
     * from 0 to 1 (0 is not included). Use temp closer to 0 for analytical /
     * multiple choice, and temp closer to 1 for creative
     * and generative tasks. Defaults to 0.95.
     */
    temperature?: number;
    /** Total probability mass of tokens to consider at each step. Range
     * from 0 to 1.0. Defaults to 0.8.
     */
    topP?: number;
    /** Penalizes repeated tokens according to frequency. Range
     * from 1.0 to 2.0. Defaults to 1.0.
     */
    penaltyScore?: number;
}
/**
 * Wrapper around Baidu ERNIE large language models that use the Chat endpoint.
 *
 * To use you should have the `QIANFAN_AK` and `QIANFAN_SK`
 * environment variable set.
 *
 * @augments BaseLLM
 * @augments BaiduERNIEInput
 * ```
 */
export declare class ChatBaiduQianfan extends BaseChatModel implements BaiduQianfanChatInput {
    static lc_name(): string;
    get callKeys(): string[];
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): {
        [key: string]: string;
    } | undefined;
    lc_serializable: boolean;
    streaming: boolean;
    prefixMessages?: Qianfan[];
    userId?: string;
    modelName: string;
    model: string;
    temperature?: number | undefined;
    topP?: number | undefined;
    penaltyScore?: number | undefined;
    client?: any;
    qianfanAK?: string;
    qianfanSK?: string;
    qianfanAccessKey?: string;
    qianfanSecretKey?: string;
    constructor(fields?: Partial<BaiduQianfanChatInput> & BaseChatModelParams);
    /**
     * Get the parameters used to invoke the model
     */
    invocationParams(): Omit<ChatCompletionRequest, "messages">;
    /**
     * Get the identifying parameters for the model
     */
    identifyingParams(): {
        system?: string | undefined;
        temperature?: number | undefined;
        stream?: boolean | undefined;
        user_id?: string | undefined;
        top_p?: number | undefined;
        penalty_score?: number | undefined;
        model_name: string;
    };
    private _ensureMessages;
    /** @ignore */
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    /** @ignore */
    completionWithRetry(request: ChatCompletionRequest, stream: boolean): Promise<ChatCompletionResponse | AsyncIterableIterator<ChatCompletionResponse>>;
    _streamResponseChunks(messages: BaseMessage[], _options?: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _llmType(): string;
}
export {};
